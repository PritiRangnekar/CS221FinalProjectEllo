# -*- coding: utf-8 -*-
"""ReadabilityAndBleuWithNFT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SddREidr7U3Sd1_C_XIsHYydPtjf_KLZ
"""

pip install textstat

!pip install -U nltk

import csv
import pandas as pd
import numpy as np
import textstat
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

# All Original Data 
batchOneTrainData = pd.read_csv ('batchOneTrain.csv', usecols=['Text', 'Commentary'])
batchOneValData = pd.read_csv ('batchOneVal.csv', usecols=['Text', 'Commentary'])
batchOneTestData = pd.read_csv ('batchOneTest.csv', usecols=['Text', 'Commentary'])

# All Summarizer Data Without Context
batchOneValSummarizerData = pd.read_csv ('summaries_batchOneVal.csv', usecols=['Text', 'Summary'])
batchOneValSummarizerData.columns = ['Text', 'Commentary']
batchOneTestSummarizerData = pd.read_csv ('summaries_batchOneTest.csv', usecols=['Text', 'Summary'])
batchOneTestSummarizerData.columns = ['Text', 'Commentary']

# All GPT2 Data Without Context
batchOneValGPT2Data = pd.read_csv ('GPT2Commentaries_batchOneVal.csv', usecols=['Text', 'Commentary'])
batchOneTestGPT2Data = pd.read_csv ('GPT2Commentaries_batchOneTest.csv', usecols=['Text', 'Commentary'])

# All BART Data Without Context
batchOneValBARTData = pd.read_csv ('BARTCommentaries_batchOneVal.csv', usecols=['Text', 'Commentary'])
batchOneTestBARTData = pd.read_csv ('BARTCommentaries_batchOneTest.csv', usecols=['Text', 'Commentary'])

################################################### WITH CONTEXT

# All Summarizer Data With Context
batchOneValSummarizerData2 = pd.read_csv ('summaries_contextb1Val.csv', usecols=['Text', 'Summary'])
batchOneValSummarizerData2.columns = ['Text', 'Commentary']
batchOneTestSummarizerData2 = pd.read_csv ('summaries_contextb1Test.csv', usecols=['Text', 'Summary'])
batchOneTestSummarizerData2.columns = ['Text', 'Commentary']

# All GPT2 Data With Context
batchOneValGPT2Data2 = pd.read_csv ('GPTCommentaries_batchOneVal2.csv', usecols=['Text', 'Commentary'])
batchOneTestGPT2Data2 = pd.read_csv ('GPTCommentaries_batchOneTest2.csv', usecols=['Text', 'Commentary'])

# All BART Data With Context
batchOneValBARTData2 = pd.read_csv ('BARTCommentaries_batchOneVal2.csv', usecols=['Text', 'Commentary'])
batchOneTestBARTData2 = pd.read_csv ('BARTCommentaries_batchOneTest2.csv', usecols=['Text', 'Commentary'])

# All Non-Fine-Tuned GPT Data With Context
batchOneValNFTGPT2Data2 = pd.read_csv ('NoFineTuneGPTVal.csv', usecols=['Text', 'Commentary'], quoting=csv.QUOTE_NONE, encoding='utf-8')
batchOneTestNFTGPT2Data2 = pd.read_csv ('NoFineTuneGPTTest.csv', usecols=['Text', 'Commentary'], quoting=csv.QUOTE_NONE, encoding='utf-8')

# All Non-Fine-Tuned BART Data With Context
batchOneValNFTBARTData2 = pd.read_csv ('NoFineTuneBartVal.csv', usecols=['Text', 'Commentary'], quoting=csv.QUOTE_NONE, encoding='utf-8')
batchOneTestNFTBARTData2 = pd.read_csv ('NoFineTuneBartTest.csv', usecols=['Text', 'Commentary'], quoting=csv.QUOTE_NONE, encoding='utf-8')

"""Readability Score Analysis"""

def getReadabilityScores(dataParam):
  readabilityScoresCommentary = []
  for row in dataParam.loc[:, ['Text', 'Commentary']].itertuples():
    commentaryForThisRow = str(row[2])
    if "Buckminster Fuller" in commentaryForThisRow:
      commentaryForThisRow = commentaryForThisRow[:len(commentaryForThisRow) - 3]
    readabilityScore = textstat.flesch_reading_ease(commentaryForThisRow)
    print(row[0], row[2], readabilityScore)
    readabilityScoresCommentary.append(readabilityScore)
  return readabilityScoresCommentary


# Average Readability Score for Original
print("Original")
print(sum(getReadabilityScores(batchOneValData)) / len(getReadabilityScores(batchOneValData)))
print(sum(getReadabilityScores(batchOneTestData)) / len(getReadabilityScores(batchOneTestData)))

# Average Readability Score for Summarizer
print("Summarizer")
print(sum(getReadabilityScores(batchOneValSummarizerData)) / len(getReadabilityScores(batchOneValSummarizerData)))
print(sum(getReadabilityScores(batchOneTestSummarizerData)) / len(getReadabilityScores(batchOneTestSummarizerData)))

# Average Readability Score for GPT2
print("GPT")
print(sum(getReadabilityScores(batchOneValGPT2Data)) / len(getReadabilityScores(batchOneValGPT2Data)))
print(sum(getReadabilityScores(batchOneTestGPT2Data)) / len(getReadabilityScores(batchOneTestGPT2Data)))

# Average Readability Score for BART
print("BART")
print(sum(getReadabilityScores(batchOneValBARTData)) / len(getReadabilityScores(batchOneValBARTData)))
print(sum(getReadabilityScores(batchOneTestBARTData)) / len(getReadabilityScores(batchOneTestBARTData)))

########################################################################### With Context
# Average Readability Score for Summarizer
print("Summarizer With Context")
print(sum(getReadabilityScores(batchOneValSummarizerData2)) / len(getReadabilityScores(batchOneValSummarizerData2)))
print(sum(getReadabilityScores(batchOneTestSummarizerData2)) / len(getReadabilityScores(batchOneTestSummarizerData2)))

# Average Readability Score for GPT2
print("GPT With Context")
print(sum(getReadabilityScores(batchOneValGPT2Data2)) / len(getReadabilityScores(batchOneValGPT2Data2)))
print(sum(getReadabilityScores(batchOneTestGPT2Data2)) / len(getReadabilityScores(batchOneTestGPT2Data2)))


# Average Readability Score for BART
print("BART With Context")
print(sum(getReadabilityScores(batchOneValBARTData2)) / len(getReadabilityScores(batchOneValBARTData2)))
print(sum(getReadabilityScores(batchOneTestBARTData2)) / len(getReadabilityScores(batchOneTestBARTData2)))

# Average Readability Score for GPT2 NFT
print("GPT NFT")
print(sum(getReadabilityScores(batchOneValNFTGPT2Data2)) / len(getReadabilityScores(batchOneValNFTGPT2Data2)))
print(sum(getReadabilityScores(batchOneTestNFTGPT2Data2)) / len(getReadabilityScores(batchOneTestNFTGPT2Data2)))

# Average Readability Score for BART NFT
print("BART NFT")
print(sum(getReadabilityScores(batchOneValNFTBARTData2)) / len(getReadabilityScores(batchOneValNFTBARTData2)))
print(sum(getReadabilityScores(batchOneTestNFTBARTData2)) / len(getReadabilityScores(batchOneTestNFTBARTData2)))

def getReadabilitySquareLoss(originalDataReadabilityScores, comparisonDataReadabilityScores):
  sum = 0
  n = len(originalDataReadabilityScores)
  for i in range(n):
    print(comparisonDataReadabilityScores[i], originalDataReadabilityScores[i])
    sum += (comparisonDataReadabilityScores[i] - originalDataReadabilityScores[i])**2
  sum /= n
  return sum


# Evaluating Summarizer
print("Summarizer")
print(getReadabilitySquareLoss(getReadabilityScores(batchOneValData), getReadabilityScores(batchOneValSummarizerData)))
print(getReadabilitySquareLoss(getReadabilityScores(batchOneTestData), getReadabilityScores(batchOneTestSummarizerData)))

# Evaluating GPT2
print("GPT")
print(getReadabilitySquareLoss(getReadabilityScores(batchOneValData), getReadabilityScores(batchOneValGPT2Data)))
print(getReadabilitySquareLoss(getReadabilityScores(batchOneTestData), getReadabilityScores(batchOneTestGPT2Data)))

# Evaluating BART
print("BART")
print(getReadabilitySquareLoss(getReadabilityScores(batchOneValData), getReadabilityScores(batchOneValBARTData)))
print(getReadabilitySquareLoss(getReadabilityScores(batchOneTestData), getReadabilityScores(batchOneTestBARTData)))


############################## WITH CONTEXT
# Evaluating Summarizer
print("Summarizer With Context")
print(getReadabilitySquareLoss(getReadabilityScores(batchOneValData), getReadabilityScores(batchOneValSummarizerData2)))
print(getReadabilitySquareLoss(getReadabilityScores(batchOneTestData), getReadabilityScores(batchOneTestSummarizerData2)))

# Evaluating GPT2
print("GPT With Context")
print(getReadabilitySquareLoss(getReadabilityScores(batchOneValData), getReadabilityScores(batchOneValGPT2Data2)))
print(getReadabilitySquareLoss(getReadabilityScores(batchOneTestData), getReadabilityScores(batchOneTestGPT2Data2)))

# Evaluating BART
print("BART With Context")
print(getReadabilitySquareLoss(getReadabilityScores(batchOneValData), getReadabilityScores(batchOneValBARTData2)))
print(getReadabilitySquareLoss(getReadabilityScores(batchOneTestData), getReadabilityScores(batchOneTestBARTData2)))


# Evaluating GPT NFT
print("GPT NFT")
print(getReadabilitySquareLoss(getReadabilityScores(batchOneValData), getReadabilityScores(batchOneValNFTGPT2Data2)))
print(getReadabilitySquareLoss(getReadabilityScores(batchOneTestData), getReadabilityScores(batchOneTestNFTGPT2Data2)))

# Evaluating BART NFT
print("BART NFT")
print(getReadabilitySquareLoss(getReadabilityScores(batchOneValData), getReadabilityScores(batchOneValNFTBARTData2)))
print(getReadabilitySquareLoss(getReadabilityScores(batchOneTestData), getReadabilityScores(batchOneTestNFTBARTData2)))


"""BLEU Score Analysis"""

import  nltk.translate.bleu_score as bleu
from nltk.translate.bleu_score import SmoothingFunction

def getBLEUScores(originalData, comparisonData):
  BLEUScoreTotal = 0
  n = len(originalData.index)
  for i in range(0, n):
    originalCommentary = originalData.values[i][1]
    comparisonCommentary = comparisonData.values[i][1]
    reference = [str(originalCommentary)]
    candidate = str(comparisonCommentary)
    smoothingFunc = SmoothingFunction().method3
    BLEUScore = bleu.sentence_bleu(reference, candidate, smoothing_function=smoothingFunc)
    print(i, originalData.values[i][0], reference, candidate, BLEUScore)
    BLEUScoreTotal += BLEUScore
  BLEUScoreTotal /= n
  return BLEUScoreTotal


# Evaluating Summarizer
print("Summarizer")
print(getBLEUScores(batchOneValData, batchOneValSummarizerData))
print(getBLEUScores(batchOneTestData, batchOneTestSummarizerData))

# Evaluating GPT2
print("GPT")
print(getBLEUScores(batchOneValData, batchOneValGPT2Data))
print(getBLEUScores(batchOneTestData, batchOneTestGPT2Data))

# Evaluating BART
print("BART")
print(getBLEUScores(batchOneValData, batchOneValBARTData))
print(getBLEUScores(batchOneTestData, batchOneTestBARTData))


################################ WITH CONTEXT
# Evaluating Summarizer
print("Summarizer With Context")
print(getBLEUScores(batchOneValData, batchOneValSummarizerData2))
print(getBLEUScores(batchOneTestData, batchOneTestSummarizerData2))

# Evaluating GPT2
print("GPT With Context")
print(getBLEUScores(batchOneValData, batchOneValGPT2Data2))
print(getBLEUScores(batchOneTestData, batchOneTestGPT2Data2))


# Evaluating BART
print("BART With Context")
print(getBLEUScores(batchOneValData, batchOneValBARTData2))
print(getBLEUScores(batchOneTestData, batchOneTestBARTData2))

# Evaluating GPT2 NFT
print("GPT NFT")
print(getBLEUScores(batchOneValData, batchOneValNFTGPT2Data2))
print(getBLEUScores(batchOneTestData, batchOneTestNFTGPT2Data2))

# Evaluating BART NFT
print("BART NFT")
print(getBLEUScores(batchOneValData, batchOneValNFTBARTData2))
print(getBLEUScores(batchOneTestData, batchOneTestNFTBARTData2))



"""METEOR"""

import nltk.translate.meteor_score as meteor

def removePunctuationFromSentence(sentence):
  punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''
  no_punct = ""
  for char in sentence:
    if char not in punctuations:
        no_punct = no_punct + char
  return no_punct

def getMETEORScores(originalData, comparisonData):
  METEORScoreTotal = 0
  n = len(originalData.index)
  for i in range(0, n):
    originalCommentary = removePunctuationFromSentence(str(originalData.values[i][1])) # punctuation is removed so that blue equiv. to blue!
    comparisonCommentary = removePunctuationFromSentence(str(comparisonData.values[i][1]))
    reference = originalCommentary.split()
    candidate = comparisonCommentary.split()
    METEORScore = meteor.single_meteor_score(reference, candidate)
    print(i, originalData.values[i][0], reference, candidate, METEORScore)
    METEORScoreTotal += METEORScore
  METEORScoreTotal /= n
  return METEORScoreTotal


#@title
# Evaluating Summarizer
print("Summarizer")
print(getMETEORScores(batchOneValData, batchOneValSummarizerData))
print(getMETEORScores(batchOneTestData, batchOneTestSummarizerData))

# Evaluating GPT2
print("GPT")
print(getMETEORScores(batchOneValData, batchOneValGPT2Data))
print(getMETEORScores(batchOneTestData, batchOneTestGPT2Data))

# Evaluating BART
print("BART")
print(getMETEORScores(batchOneValData, batchOneValBARTData))
print(getMETEORScores(batchOneTestData, batchOneTestBARTData))

####################################### WITH CONTEXT
# Evaluating Summarizer
print("Summarizer With Context")
print(getMETEORScores(batchOneValData, batchOneValSummarizerData2))
print(getMETEORScores(batchOneTestData, batchOneTestSummarizerData2))

# Evaluating GPT2
print("GPT With Context")
print(getMETEORScores(batchOneValData, batchOneValGPT2Data2))
print(getMETEORScores(batchOneTestData, batchOneTestGPT2Data2))

# Evaluating BART
print("BART With Context")
print(getMETEORScores(batchOneValData, batchOneValBARTData2))
print(getMETEORScores(batchOneTestData, batchOneTestBARTData2))

# Evaluating GPT2 NFT
print("GPT  NFT")
print(getMETEORScores(batchOneValData, batchOneValNFTGPT2Data2))
print(getMETEORScores(batchOneTestData, batchOneTestNFTGPT2Data2))

# Evaluating BART NFT
print("BART NFT")
print(getMETEORScores(batchOneValData, batchOneValNFTBARTData2))
print(getMETEORScores(batchOneTestData, batchOneTestNFTBARTData2)

